{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition of images of the fashion mnist dataset\n",
    "\n",
    "## The project\n",
    "\n",
    "\n",
    "The aim of the project is to use differents techniques of deep learning in order to predict type of clothes of the fashion mnist dataset the more precisely possible.\n",
    "\n",
    "The dataset is compose of 60 000 images for training and 10 000 images for testing.\n",
    "\n",
    "<img src=\"img/Fashion-MNIST-Dataset-Images-with-Labels-and-Description.png\">\n",
    "\n",
    "There are 10 different classes, the neural network will have to predict for an image given what type of class it is.\n",
    "\n",
    "## The code\n",
    "\n",
    "### Linear model\n",
    "\n",
    "We will begin our training by our more simple model : a linear model.\n",
    "\n",
    "First we need to import the packages we will need :\n",
    "\n",
    "```python\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "```\n",
    "\n",
    "Then we need to create our linear model using keras :\n",
    "\n",
    "```python\n",
    "\n",
    "def linear_model(x, y, val_x, val_y, opt, loss_func, epochs, batch_size):\n",
    "    model = keras.Sequential([\n",
    "        # convert a two dimensional matrix into a vector\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=keras.activations.softmax),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss_func, metrics=keras.metrics.categorical_accuracy)\n",
    "\n",
    "    logs = model.fit(x, y, validation_data=(val_x, val_y), epochs=epochs, batch_size=batch_size,\n",
    "                     callbacks=[keras.callbacks.LearningRateScheduler(scheduler)])\n",
    "    model.summary()\n",
    "\n",
    "    return logs\n",
    "\n",
    "```\n",
    "\n",
    "The model take in parameter :\n",
    "\n",
    "* The training and testing datas\n",
    "* The function of optimization\n",
    "* The function for evaluate the loss\n",
    "* The epochs (number of time the neural network process the entire datset)\n",
    "* The batch size (number of example given before the neural network corrige the weights\n",
    "\n",
    "Here we choose for the activation function the softmax because the sum of the output returned is 1 and its good in a categorical problem as it return a pourcentage on how much it thinks an image is a certain type of category or not.\n",
    "\n",
    "When we fit the model there is an argument called callbacks, what does he do ? This argument call every epochs the function scheduler :\n",
    "\n",
    "```python\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 150:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9875\n",
    "\n",
    "```\n",
    "\n",
    "This function allow the learning rate to be reduced from the 150th iteration. Reduction of the learning rate will allow the neural network to become more and more precise between each epoch(from the 150th).\n",
    "\n",
    "\n",
    "In the main function :\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # how many time the model will review the training data\n",
    "    epochs = 300\n",
    "    # number of data images who spreed through the network (forward propagation), after that the network\n",
    "    # mean the sum of errors and make only one backpropagation\n",
    "    # batch size increase the available computational parallelism and make it converge faster to optimum local\n",
    "    # but algorithm with large batch size will hardly find the minimum global compared to little bach size\n",
    "    batch_size = 1024\n",
    "\n",
    "    # get data of training and testing from fashion mnist dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "    # pixel have values from 0 to 255, normalize them\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    # transform label (containing a value from O to 9) to matrix of 10 (one hot encoding)\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    all_logs = []\n",
    "    log = linear_model(x_train, y_train, x_test, y_test, keras.optimizers.SGD(lr=0.05, momentum=0.95),\n",
    "                       keras.losses.categorical_crossentropy, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    all_logs.append(log)\n",
    "\n",
    "    plot_log(all_logs)\n",
    "    \n",
    "\n",
    "```\n",
    "So firstly we introduce hyperparameters epochs and batch_size and set it respectively to 300 and 1024.\n",
    "A large batch size will allow the network to process the data much faster but there at risk that it converge in global (and not local) optimum.\n",
    "\n",
    "For the loss function, cross-entropy is used as it is a good function coupled to the softmax functions as it penalized well the deviations between output and predicted values.\n",
    "\n",
    "The function plot_log allow us to display the loss and accuracy of our models.\n",
    "\n",
    "After 70 epochs, here are our results :\n",
    "\n",
    "<img src=\"img/plot_1_1.png\">\n",
    "\n",
    "<img src=\"img/plot_1_2.png\">\n",
    "\n",
    "<img src=\"img/plot_1_3.png\">\n",
    "\n",
    "<img src=\"img/plot_1_4.png\">\n",
    "\n",
    "As we can see in these graphs, the loss fall down in the first epochs and then decrease a little.\n",
    "On the training data, the values seems to decrease until the end but on the training data the loss seems to stabilise and even increase at the end. Let's look more carrefuly the datas :\n",
    "\n",
    "<img src=\"img/plot_1_5.PNG\">\n",
    "\n",
    "<img src=\"img/plot_1_6.PNG\">\n",
    "\n",
    "<img src=\"img/plot_1_7.PNG\">\n",
    "\n",
    "As we can see, the loss on the testing test was as its lowest on the 38th epoch. Then the loss on training test is still increasing a little but the loss on training test decrease over the time.\n",
    "\n",
    "It suggets that we are strating overfitting, the model start over-learn and can't generalize anymore.\n",
    "\n",
    "\n",
    "Let's try with a MLP\n",
    "\n",
    "\n",
    "\n",
    "### Multi Layer Perceptron\n",
    "\n",
    "\n",
    "The only difference with the previous code is that we add two more layers in the model :\n",
    "\n",
    "```python3\n",
    "\n",
    "def multi_layer_perceptron(x, y, val_x, val_y, opt, loss_func, epochs, batch_size):\n",
    "    model = keras.Sequential([\n",
    "        # convert a two dimensional matrix into a vector\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(60, activation=keras.activations.relu),\n",
    "        keras.layers.Dense(60, activation=keras.activations.relu),\n",
    "        keras.layers.Dense(10, activation=keras.activations.softmax),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss_func, metrics=keras.metrics.categorical_accuracy)\n",
    "\n",
    "    logs = model.fit(x, y, validation_data=(val_x, val_y), epochs=epochs, batch_size=batch_size,\n",
    "                     callbacks=[keras.callbacks.LearningRateScheduler(scheduler)])\n",
    "    model.summary()\n",
    "\n",
    "    return logs\n",
    "\n",
    "```\n",
    "\n",
    "I reduce the number of epochs at 50 as he become useless to train more if the model overfit before the end.\n",
    "\n",
    "I also change the scheduler function :\n",
    "\n",
    "```python3\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 30:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.98\n",
    "\n",
    "```\n",
    "\n",
    "Let's see what are the results :\n",
    "\n",
    "\n",
    "<img src=\"img/plot_2_1.png\">\n",
    "\n",
    "<img src=\"img/plot_2_2.png\">\n",
    "\n",
    "<img src=\"img/plot_2_3.png\">\n",
    "\n",
    "<img src=\"img/plot_2_4.png\">\n",
    "\n",
    "\n",
    "As we can see with the plots, the MLP performs much better than the previous model.\n",
    "For these example i use the activation function relu, what would happen with others activation functions ? Which is the best for this example ? We will see :\n",
    "\n",
    "\n",
    "<img src=\"img/plot_3_1.png\">\n",
    "\n",
    "<img src=\"img/plot_3_2.png\">\n",
    "\n",
    "<img src=\"img/plot_3_3.png\">\n",
    "\n",
    "<img src=\"img/plot_3_4.png\">\n",
    "\n",
    "\n",
    "We can see very interesting results, elu, relu, selu and tanh activation function seems pretty similar. Otherwise the sigmoid function tends to work poorly on the earlys epochs but at the end started to surpass the others.\n",
    "\n",
    "\n",
    "We can see by analysing the error on training and testinf that the model is overfitting the data except on sigmoid.\n",
    "We have 3 ways to fight overfitting :\n",
    "* reduce the model complexity\n",
    "* add more data\n",
    "* add regularization technics\n",
    "\n",
    "Adding more data is not possible and between the two last choices i choose to add regularization technics.\n",
    "let's see what happens if i add dropout.\n",
    "\n",
    "Dropout is a technic that select random neurons who will be ignored during training. Dropout is use because it reduce the variance of the data.\n",
    "\n",
    "I have also added a new callback named EarlyStopping, this callback has few advantages :\n",
    "\n",
    "* prevent the model to overfit by stopping it when it stop improving\n",
    "* gain calculuse time by stopping it before the end\n",
    "\n",
    "Let's see the results :\n",
    "\n",
    "<img src=\"img/plot_4_1.png\">\n",
    "\n",
    "<img src=\"img/plot_4_2.png\">\n",
    "\n",
    "<img src=\"img/plot_4_3.png\">\n",
    "\n",
    "<img src=\"img/plot_4_4.png\">\n",
    "\n",
    "As we can see, our models have begin to overfit way much atfer that in our previous cases but the performance has not increase.\n",
    "Let's try to change the architecture of the MLP and pass from 2 to 3 or more hidden layers.\n",
    "\n",
    "With the arhitecture :\n",
    "* 28 * 28 neurons as inputs\n",
    "* first hidden layer 128 neurons\n",
    "* second hidden layer 64 neurons\n",
    "* third hidden layer 32 neurons\n",
    "* 10 neurons as outputs\n",
    "\n",
    "I obtained the following results : \n",
    "\n",
    "<img src=\"img/plot_5_1.png\">\n",
    "\n",
    "<img src=\"img/plot_5_2.png\">\n",
    "\n",
    "<img src=\"img/plot_5_3.png\">\n",
    "\n",
    "<img src=\"img/plot_5_4.png\">\n",
    "\n",
    "As we can see there are not a lot of changes, finally let's try take the adam optimizer, Adam optimize use adaptive learning rate that allow programs to converge faster on local minima\n",
    "\n",
    "<img src=\"img/plot_6_1.png\">\n",
    "\n",
    "<img src=\"img/plot_6_2.png\">\n",
    "\n",
    "<img src=\"img/plot_6_3.png\">\n",
    "\n",
    "<img src=\"img/plot_6_4.png\">\n",
    "\n",
    "So, we converge faster but obtain slightly worse results.\n",
    "\n",
    "I try to change parameters like lower learning rate but finally we lose the advantage of the fatest convergence.\n",
    "Adding a fourth layer doesn't help much\n",
    "\n",
    "So if we look our previous results with stochastic gradient descent, the sigmoid function and selu converge too much lower for pretty much same results on testing data than the others so we remove it.\n",
    "\n",
    "Now lets run various tests and see the result on testing test:\n",
    "\n",
    "<img src=\"img/plot_7_1.png\">\n",
    "\n",
    "<img src=\"img/plot_7_2.png\">\n",
    "\n",
    "<img src=\"img/plot_7_3.png\">\n",
    "\n",
    "<img src=\"img/plot_7_4.png\">\n",
    "\n",
    "Between the three functions, they seems to are pretty the same (first graph relu win, second elu win, third equal and fourth is for tanh) but the time for converging is better for relu so relu seems better in our case.\n",
    "\n",
    "Testing a lot of time the same algorithm is important because patterns ca be different, for example i test a little more and obtain different results : \n",
    "\n",
    "<img src=\"img/plot_8_1.png\">\n",
    "\n",
    "<img src=\"img/plot_8_2.png\">\n",
    "\n",
    "<img src=\"img/plot_8_3.png\">\n",
    "\n",
    "As we see in the first elu was the fastest and the lowest ton converge in second and third graph. It's due to weights initialisation.\n",
    "\n",
    "After more testing by variate number of size of hidden layers and change dropout i found what one of the best infrastructure for the model :\n",
    "\n",
    "* 2 hiddens layers of 120 neurons\n",
    "* relu activation function (as it give best results with tanh and is the one who converge the fatest\n",
    "* dropout of 20 %\n",
    "\n",
    "Let's see his comparaison with our previous model the linear model (multi perceptron without hidden layers) :\n",
    "\n",
    "<img src=\"img/plot_9_1.png\">\n",
    "\n",
    "<img src=\"img/plot_9_2.png\">\n",
    "\n",
    "<img src=\"img/plot_9_3.png\">\n",
    "\n",
    "<img src=\"img/plot_9_4.png\">\n",
    "\n",
    "As we see our MLP performs much better (89,46 % at his peak) against linear model (84,62%)\n",
    "\n",
    "\n",
    "### Convolutional neural network\n",
    "\n",
    "\n",
    "Nows let's try a last and different approach, instead of just process the image directly in the MLP we will before apply to her some changes : It's the convolutional part and pooling part.\n",
    "\n",
    "<img src=\"img/convolution_illustration.png\">\n",
    "\n",
    "As we see the image had a sucession of convolution and pooling layer then the data is process into a fully connected layer and the output is a vector with the % of each class (softmax function)\n",
    "\n",
    "So firstly what is the convolution ?\n",
    "\n",
    "Convolution is use to extract the features of the images.\n",
    "\n",
    "Let's see for example the edge filter :\n",
    "\n",
    "<img src=\"img/filter_illustration.jpg\">\n",
    "\n",
    "As we can see we apply to the 2 images 2 filters, 1 for vertical edge detection and one other for horizontal edge detection.\n",
    "And we obtain as output the same image with in blank the edge detected wether horizontal or vertical.\n",
    "\n",
    "After extracting features we pass the images resulting into a pooling part.\n",
    "\n",
    "The job of a pooling layer is to reduce the number of pixels of the image in order to decrease the computanional power requires to process data.\n",
    "\n",
    "There are 2 types of pooling layer : \n",
    "\n",
    "* The mean pooling\n",
    "* The max pooling\n",
    "\n",
    "<img src=\"img/pooling_illustration.png\">\n",
    "\n",
    "In our example giving a matrix of 8 * 8 pixels and a step of 2 we will obtain a matrix of 4 * 4 pixels.\n",
    "\n",
    "\n",
    "Let's see the effect of max pooling on real images.\n",
    "\n",
    "<img src=\"img/before_polling.jpg\">\n",
    "\n",
    "<img src=\"img/after_pooling.jpg\">\n",
    "\n",
    "As we can see, we reduce the size of the image without losing so much information.\n",
    "\n",
    "Theses images are from the folder named \"without_tensor_flow\".\n",
    "If you want to see more about the algorithms used on convolution and pooling layers i recommend you to check the python files \"Convolution.py\" and \"Pooling.py\".\n",
    "\n",
    "After that the resulting images are put into a fully connected layer (MLP)\n",
    "\n",
    "Let's implement that in TensorFlow\n",
    "\n",
    "```python\n",
    "\n",
    "def convolutional_neural_network(x, y, val_x, val_y, opt, loss_func, epochs, batch_size, activation, dropout):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Reshape((28, 28, 1)),\n",
    "\n",
    "        keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=activation),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Dropout(dropout),\n",
    "\n",
    "        keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=activation),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Dropout(dropout),\n",
    "\n",
    "        keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=activation),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Dropout(dropout),\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "\n",
    "        keras.layers.Dense(10, activation=keras.activations.softmax)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss_func, metrics=keras.metrics.categorical_accuracy)\n",
    "\n",
    "    logs = model.fit(x, y, validation_data=(val_x, val_y), epochs=epochs, batch_size=batch_size,\n",
    "                     callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "I have first tried with a linear model at the end with no hidden layers, let's see the results : \n",
    "\n",
    "<img src=\"img/plot_10_1.png\">\n",
    "\n",
    "<img src=\"img/plot_10_2.png\">\n",
    "\n",
    "<img src=\"img/plot_10_3.png\">\n",
    "\n",
    "<img src=\"img/plot_10_4.png\">\n",
    "\n",
    "As we can see except for sigmoid results are pretty similar and higher than those obtained with a MLP.\n",
    "We can also see that there is no overfitting so let's complify a bit more the model by adding hidden couch.\n",
    "\n",
    "Let's see now what are the results with a hidden couch of 60 neurons, i keep sigmoid in case of a miracle appear and sigmoid work amazingly with the hidden layer.\n",
    "\n",
    "<img src=\"img/plot_11_1.png\">\n",
    "\n",
    "<img src=\"img/plot_11_2.png\">\n",
    "\n",
    "<img src=\"img/plot_11_3.png\">\n",
    "\n",
    "<img src=\"img/plot_11_4.png\">\n",
    "\n",
    "The model runs better without the hidden layer, so let's try with only 30 neurons in the hidden layer.\n",
    "\n",
    "<img src=\"img/plot_12_1.png\">\n",
    "\n",
    "<img src=\"img/plot_12_2.png\">\n",
    "\n",
    "<img src=\"img/plot_12_3.png\">\n",
    "\n",
    "<img src=\"img/plot_12_4.png\">\n",
    "\n",
    "Here the results are better than ever, let's see what happens if we reduce the dropout from 20 to 10 % :\n",
    "\n",
    "<img src=\"img/plot_13_1.png\">\n",
    "\n",
    "<img src=\"img/plot_13_2.png\">\n",
    "\n",
    "<img src=\"img/plot_13_3.png\">\n",
    "\n",
    "<img src=\"img/plot_13_4.png\">\n",
    "\n",
    "With 10 % of dropout, we obtain the best results with a pic of 92.3 % on testing test for relu on the 79th epoch.\n",
    "As the tanh and relu gives the best performances, we will train our models only on them, that he will allow us to train during more epochs. Let's run them both relu and tanh on 150 epochs and see what happens :\n",
    "\n",
    "<img src=\"img/plot_14_1.png\">\n",
    "\n",
    "<img src=\"img/plot_14_2.png\">\n",
    "\n",
    "<img src=\"img/plot_14_3.png\">\n",
    "\n",
    "<img src=\"img/plot_14_4.png\">\n",
    "\n",
    "Relu seems to give the best results compared to tanh, let's see what % of dropout is the best for relu :\n",
    "\n",
    "\n",
    "<img src=\"img/plot_15_1.png\">\n",
    "\n",
    "<img src=\"img/plot_15_2.png\">\n",
    "\n",
    "<img src=\"img/plot_15_3.png\">\n",
    "\n",
    "<img src=\"img/plot_15_4.png\">\n",
    "\n",
    "Results between the thrid values are pretty similar, let's run a second test to decide which value of dropout is the best.\n",
    "\n",
    "<img src=\"img/plot_15_5.png\">\n",
    "\n",
    "I decide to keep a value of 10 % for the dropout as it gives the best values for both test.\n",
    "\n",
    "\n",
    "\n",
    "We had previously a batch size of 1024, however the value of the batch size is too high and a lower value could give best values.\n",
    "\n",
    "Let's run a test with batch size values of 128, 256 and 512 :\n",
    "\n",
    "\n",
    "<img src=\"img/plot_16_1.png\">\n",
    "\n",
    "<img src=\"img/plot_16_2.png\">\n",
    "\n",
    "<img src=\"img/plot_16_3.png\">\n",
    "\n",
    "<img src=\"img/plot_16_4.png\">\n",
    "\n",
    "As we can see, a batch size of 256 give the best results. We also see than larger batch size need more epochs to converge before earlyStopping stopped the training. But even with a larger numbers of epochs needed, they are still faster due to GPU parallelism.\n",
    "\n",
    "So, as we can see, our best results give an accuracy around 0.925.\n",
    "\n",
    "Now we will use a different technic called data augmentation, the principle is to create images variant of the training's images. \n",
    "\n",
    "There are 2 ways of reduce overfitting :\n",
    "- reduce the model complexity, but in our case it will not increase our accuracy\n",
    "- add regularisation technics, we already use dropout\n",
    "- increase the number of data, this is the data augmentation\n",
    "\n",
    "As we will increase drastically the number of data, regularization technics as dropout is no longuer required.\n",
    "Moreover, we will use another technic which is not efficient when coupled to dropout : batch normalization.\n",
    "Batch normalization as explained in the start of the notebook is used to speed-up the training, but the difference is that normalization is used on training data and batch normalization is uded on convolutions matrix.\n",
    "\n",
    "Batch normalization also have a slight effect of normalization as it add noise to the datas because the batch normalization is effected on batches and not the overall data. It can reduce covariance shift problems.\n",
    "\n",
    "We use Batch normalization after each convolutional part.\n",
    "\n",
    "The data augmentation constist of produce new images from on existing image frome the training dataset.\n",
    "The image will have modifications as :\n",
    "- Apply a symmetry to the image (horizontal or vertical, vertical in our case)\n",
    "- The image is zoomed\n",
    "- The image have a random rotation (not useful in our case beacause images are always centered)\n",
    "- ...\n",
    "\n",
    "This article explained very well the various parameters for data augmentation : https://towardsdatascience.com/exploring-image-data-augmentation-with-keras-and-tensorflow-a8162d89b844\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "train_generator = dataGen_training.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "x_valid = x_train[:150 * batch_size]\n",
    "y_valid = y_train[:150 * batch_size]\n",
    "\n",
    "valid_steps = x_valid.shape[0] // batch_size\n",
    "validation_generator = dataGen_testing.flow(x_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Firstly, i create arrays who are 150 times larger than the dataset for training, meaning that for each image of the dataset, 149 variants will be created.\n",
    "\n",
    "Here is the architecture i use :\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "        Reshape((28, 28, 1)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv2D(196, (3, 3), padding=\"same\", activation=activation),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(196, (3, 3), padding=\"same\", activation=activation),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(),\n",
    "\n",
    "        Conv2D(92, (3, 3), padding=\"same\", activation=activation),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(92, (3, 3), padding=\"same\", activation=activation),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(),\n",
    "\n",
    "        Conv2D(48, (3, 3), padding=\"same\", activation=activation),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(48, (3, 3), padding=\"same\", activation=activation),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "\n",
    "        keras.layers.Dense(30, activation=activation),\n",
    "\n",
    "        BatchNormalization(),\n",
    "\n",
    "        keras.layers.Dense(10, activation=keras.activations.softmax)\n",
    "    ])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "For an unknow reason, 2 convolution layer with half size works better than 1 convolution layer.\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.02), loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=keras.metrics.categorical_accuracy)\n",
    "\n",
    "logs = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(x_train) // batch_size,\n",
    "    epochs=200,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "               keras.callbacks.LearningRateScheduler(scheduler)],\n",
    "    validation_data=validation_generator,\n",
    "    validation_freq=1,\n",
    "    validation_steps=valid_steps,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "I used Adam, it is a good optimizer who need no tuning and give good results and a very fast convergence.\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "dataGen_training = ImageDataGenerator(\n",
    "  rotation_range=10,\n",
    "  horizontal_flip=False,\n",
    "  vertical_flip = True,\n",
    "  width_shift_range=0.1,\n",
    "  height_shift_range=0.1,\n",
    "  rescale=1. / 255,\n",
    "  shear_range=0.05,\n",
    "  zoom_range=0.05,\n",
    ")\n",
    "\n",
    "dataGen_testing = ImageDataGenerator(\n",
    "  rescale=1. / 255,\n",
    ")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Here is the code for data augmentation, i made some **HUGES mistakes**\n",
    "At the time i write the code i thougt vertical flip mean vertical symmetry, moreover i copy cut the other lines from my study on CIFAR10 Dataset that you can found here (in french) : https://github.com/eldoria/cifar10/blob/master/Recognition%20of%20image%20on%20cifar-10.ipynb\n",
    "As all the images are centred, shift_range, shear_range and zoom_range are useless and even counter productive.\n",
    "\n",
    "The testing is not modified as we must never touch the testing dataset otherwise we will not evaluate properly our model.\n",
    "\n",
    "\n",
    "Even with this bad configuration i have fantastic results :\n",
    "\n",
    "\n",
    "<img src=\"img/plot_17_1.png\">\n",
    "\n",
    "<img src=\"img/plot_17_2.png\">\n",
    "\n",
    "<img src=\"img/plot_17_3.png\">\n",
    "\n",
    "<img src=\"img/plot_17_4.png\">\n",
    "\n",
    "The pic for learning rate is 0.995 for testing set and 0.992 for learning rate.\n",
    "If I had let the algorithm run more time it would have certainly atteigned near prefect score on training set, testing had more prediction than training because images in training was harder to predict as they are modified.\n",
    "\n",
    "We can see that the adam optimize make converge VERY fast, but the problem is that after a certain time testing set stop imrpving and for an obscure reason, EarlyStopping never stop the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
