{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition of images of the fashion mnist dataset\n",
    "\n",
    "## The project\n",
    "\n",
    "\n",
    "The aim of the project is to use differents techniques of deep learning in order to predict type of clothes of the fashion mnist dataset the more precisely possible.\n",
    "\n",
    "The dataset is compose of 60 000 images for training and 10 000 images for testing.\n",
    "\n",
    "<img src=\"img/Fashion-MNIST-Dataset-Images-with-Labels-and-Description.png\">\n",
    "\n",
    "There are 10 different classes, the neural network will have to predict for an image given what type of class it is.\n",
    "\n",
    "## The code\n",
    "\n",
    "### Linear model\n",
    "\n",
    "We will begin our training by our more simple model : a linear model.\n",
    "\n",
    "First we need to import the packages we will need :\n",
    "\n",
    "```python\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "```\n",
    "\n",
    "Then we need to create our linear model using keras :\n",
    "\n",
    "```python\n",
    "\n",
    "def linear_model(x, y, val_x, val_y, opt, loss_func, epochs, batch_size):\n",
    "    model = keras.Sequential([\n",
    "        # convert a two dimensional matrix into a vector\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=keras.activations.softmax),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss_func, metrics=keras.metrics.categorical_accuracy)\n",
    "\n",
    "    logs = model.fit(x, y, validation_data=(val_x, val_y), epochs=epochs, batch_size=batch_size,\n",
    "                     callbacks=[keras.callbacks.LearningRateScheduler(scheduler)])\n",
    "    model.summary()\n",
    "\n",
    "    return logs\n",
    "\n",
    "```\n",
    "\n",
    "The model take in parameter :\n",
    "\n",
    "* The training and testing datas\n",
    "* The function of optimization\n",
    "* The function for evaluate the loss\n",
    "* The epochs (number of time the neural network process the entire datset)\n",
    "* The batch size (number of example given before the neural network corrige the weights\n",
    "\n",
    "Here we choose for the activation function the softmax because the sum of the output returned is 1 and its good in a categorical problem as it return a pourcentage on how much it thinks an image is a certain type of category or not.\n",
    "\n",
    "When we fit the model there is an argument called callbacks, what does he do ? This argument call every epochs the function scheduler :\n",
    "\n",
    "```python\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 150:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9875\n",
    "\n",
    "```\n",
    "\n",
    "This function allow the learning rate to be reduced from the 150th iteration. Reduction of the learning rate will allow the neural network to become more and more precise between each epoch(from the 150th).\n",
    "\n",
    "\n",
    "In the main function :\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # how many time the model will review the training data\n",
    "    epochs = 300\n",
    "    # number of data images who spreed through the network (forward propagation), after that the network\n",
    "    # mean the sum of errors and make only one backpropagation\n",
    "    # batch size increase the available computational parallelism and make it converge faster to optimum local\n",
    "    # but algorithm with large batch size will hardly find the minimum global compared to little bach size\n",
    "    batch_size = 1024\n",
    "\n",
    "    # get data of training and testing from fashion mnist dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "    # pixel have values from 0 to 255, normalize them\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    # transform label (containing a value from O to 9) to matrix of 10 (one hot encoding)\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    all_logs = []\n",
    "    log = linear_model(x_train, y_train, x_test, y_test, keras.optimizers.SGD(lr=0.05, momentum=0.95),\n",
    "                       keras.losses.categorical_crossentropy, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    all_logs.append(log)\n",
    "\n",
    "    plot_log(all_logs)\n",
    "    \n",
    "\n",
    "```\n",
    "So firstly we introduce hyperparameters epochs and batch_size and set it respectively to 300 and 1024.\n",
    "A large batch size will allow the network to process the data much faster but there at risk that it converge in global (and not local) optimum.\n",
    "\n",
    "For the loss function, cross-entropy is used as it is a good function coupled to the softmax functions as it penalized well the deviations between output and predicted values.\n",
    "\n",
    "The function plot_log allow us to display the loss and accuracy of our models.\n",
    "\n",
    "After 70 epochs, here are our results :\n",
    "\n",
    "<img src=\"img/plot_1_1.png\">\n",
    "\n",
    "<img src=\"img/plot_1_2.png\">\n",
    "\n",
    "<img src=\"img/plot_1_3.png\">\n",
    "\n",
    "<img src=\"img/plot_1_4.png\">\n",
    "\n",
    "As we can see in these graphs, the loss fall down in the first epochs and then decrease a little.\n",
    "On the training data, the values seems to decrease until the end but on the training data the loss seems to stabilise and even increase at the end. Let's look more carrefuly the datas :\n",
    "\n",
    "<img src=\"img/plot_1_5.png\">\n",
    "\n",
    "<img src=\"img/plot_1_6.png\">\n",
    "\n",
    "<img src=\"img/plot_1_7.png\">\n",
    "\n",
    "As we can see, the loss on the testing test was as its lowest on the 38th epoch. Then the loss on training test is still increasing a little but the loss on training test decrease over the time.\n",
    "\n",
    "It suggets that we are strating overfitting, the model start over-learn and can't generalize anymore.\n",
    "\n",
    "\n",
    "Let's try with a MLP\n",
    "\n",
    "\n",
    "\n",
    "### Multi Layer Perceptron\n",
    "\n",
    "\n",
    "The only difference with the previous code is that we add two more layers in the model :\n",
    "\n",
    "```python3\n",
    "\n",
    "def multi_layer_perceptron(x, y, val_x, val_y, opt, loss_func, epochs, batch_size):\n",
    "    model = keras.Sequential([\n",
    "        # convert a two dimensional matrix into a vector\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(60, activation=keras.activations.relu),\n",
    "        keras.layers.Dense(60, activation=keras.activations.relu),\n",
    "        keras.layers.Dense(10, activation=keras.activations.softmax),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss_func, metrics=keras.metrics.categorical_accuracy)\n",
    "\n",
    "    logs = model.fit(x, y, validation_data=(val_x, val_y), epochs=epochs, batch_size=batch_size,\n",
    "                     callbacks=[keras.callbacks.LearningRateScheduler(scheduler)])\n",
    "    model.summary()\n",
    "\n",
    "    return logs\n",
    "\n",
    "```\n",
    "\n",
    "I reduce the number of epochs at 50 as he become useless to train more if the model overfit before the end.\n",
    "\n",
    "I also change the scheduler function :\n",
    "\n",
    "```python3\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 30:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.98\n",
    "\n",
    "```\n",
    "\n",
    "Let's see what are the results :\n",
    "\n",
    "\n",
    "<img src=\"img/plot_2_1.png\">\n",
    "\n",
    "<img src=\"img/plot_2_2.png\">\n",
    "\n",
    "<img src=\"img/plot_2_3.png\">\n",
    "\n",
    "<img src=\"img/plot_2_4.png\">\n",
    "\n",
    "\n",
    "As we can see with the plots, the MLP performs much better than the previous model.\n",
    "For these example i use the activation function relu, what would happen with others activation functions ? Which is the best for this example ? We will see :\n",
    "\n",
    "\n",
    "<img src=\"img/plot_3_1.PNG\">\n",
    "\n",
    "<img src=\"img/plot_3_2.PNG\">\n",
    "\n",
    "<img src=\"img/plot_3_3.PNG\">\n",
    "\n",
    "<img src=\"img/plot_3_4.PNG\">\n",
    "\n",
    "\n",
    "We can see very interesting results, elu, relu, selu and tanh activation function seems pretty similar. Otherwise the sigmoid function tends to work poorly on the earlys epochs but at the end started to surpass the others.\n",
    "\n",
    "\n",
    "We can see by analysing the error on training and testinf that the model is overfitting the data except on sigmoid.\n",
    "We have 3 ways to fight overfitting :\n",
    "* reduce the model complexity\n",
    "* add more data\n",
    "* add regularization technics\n",
    "\n",
    "Adding more data is not possible and between the two last choices i choose to add regularization technics.\n",
    "let's see what happens if i add dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
